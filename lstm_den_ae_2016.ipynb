{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lstm_den_ae_2016.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM0h1dbb+WRdWPEYo9tKRcS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhbrU2CtONU4","executionInfo":{"status":"ok","timestamp":1629546970637,"user_tz":420,"elapsed":24471,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"949a1518-4d68-4dc1-ebde-fa8528ce509b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"output_embedded_package_id":"1BgFZyPwXBtyDP53GYCKQylflXsvo2OzP"},"id":"qex3ctE8OWvU","executionInfo":{"status":"ok","timestamp":1629546977936,"user_tz":420,"elapsed":5232,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"c34b89ea-764a-44b2-a6ed-c2193182a1ed"},"source":["import os\n","os.environ[\"CUDA_DEVICE_ORDER\"] = 'PCI_BUS_ID'\n","os.environ['CUDA_VISIBLE_DEVICES']= '0'\n","import numpy as np\n","import pickle\n","import tensorflow as tf\n","import plotly.graph_objs as go\n","import matplotlib.pyplot as plt\n","from plotly.offline import init_notebook_mode, iplot\n","import time\n","import seaborn as sn\n","import pandas as pd\n","import h5py\n","init_notebook_mode()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"g7cggHXyO58c","executionInfo":{"status":"ok","timestamp":1629546982545,"user_tz":420,"elapsed":399,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}}},"source":["# parameters\n","data_path = '/content/drive/MyDrive/Colab Notebooks/Data/'\n","file_name = 'RML2016.10a_dict.pkl'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNoF8SqvPEwo","executionInfo":{"status":"ok","timestamp":1629547029913,"user_tz":420,"elapsed":185,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}}},"source":["signal_len = 128\n","modulation_num = 11\n","# Calculate the amplitude and phase of the input data\n","def get_amp_phase(data):\n","    X_train_cmplx = data[:, 0, :] + 1j * data[:, 1, :]\n","    X_train_amp = np.abs(X_train_cmplx)\n","    X_train_ang = np.arctan2(data[:, 1, :], data[:, 0, :]) / np.pi\n","    X_train_amp = np.reshape(X_train_amp, (-1, 1, signal_len))\n","    X_train_ang = np.reshape(X_train_ang, (-1, 1, signal_len))\n","    X_train = np.concatenate((X_train_amp, X_train_ang), axis = 1) \n","    X_train = np.transpose(np.array(X_train), (0, 2, 1))\n","    for i in range(X_train.shape[0]):\n","        X_train[i, :, 0] = X_train[i, :, 0] / np.linalg.norm(X_train[i, :, 0], 2)\n","    \n","    return X_train\n","#X_train.shape "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"kW1b51HGPOpa","executionInfo":{"status":"ok","timestamp":1629547048118,"user_tz":420,"elapsed":172,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}}},"source":["def set_up_data(data_path, file_name):\n","    # import data\n","    with open(data_path + file_name, 'rb') as f:\n","        data = pickle.load(f, encoding = 'latin1')\n","\n","    # get data in dictionary format (Modulaton -> SNR -> IQ)\n","    dic = {}\n","    for item in list(data):\n","        modulation = item[0]\n","        SNR = int(item[1])\n","\n","        if modulation not in dic:\n","            dic[modulation] = {SNR : data[item]}\n","        elif SNR not in dic[modulation]:\n","            dic[modulation][SNR] = data[item]\n","\n","    # build data in np array format\n","    len_feature = dic[list(dic)[0]][list(dic[list(dic)[0]])[0]][0].shape[1]\n","    data = np.empty((0, 2, len_feature), float) # 2 for IQ components\n","    label = []\n","    for modulation in dic:\n","        for snr in dic[modulation]:\n","            label.extend(list([modulation, snr] for _ in range(dic[modulation][snr].shape[0])))\n","            data = np.vstack((data, dic[modulation][snr]))\n","\n","    label = np.array(label)\n","    index  = list(range(data.shape[0]))\n","    np.random.seed(2019)\n","    np.random.shuffle(index)\n","\n","    train_proportion = 0.5\n","    validation_proportion = 0.25\n","    test_proportion = 0.25\n","\n","    X_train = data[index[:int(data.shape[0] * train_proportion)], :, :]\n","    Y_train = label[index[:int(data.shape[0] * train_proportion)]]\n","    X_validation = data[index[int(data.shape[0] * train_proportion) : int(data.shape[0] * (train_proportion + validation_proportion))], :, :]\n","    Y_validation = label[index[int(data.shape[0] * train_proportion) : int(data.shape[0] * (train_proportion + validation_proportion))]]\n","    X_test = data[index[int(data.shape[0] * (train_proportion + validation_proportion)):], :, :]\n","    Y_test = label[index[int(data.shape[0] * (train_proportion + validation_proportion)):]]\n","    \n","    modulation_index = {}\n","    modulations = np.sort(list(dic))\n","    for i in range(len(list(dic))):\n","        modulation_index[modulations[i]] = i\n","\n","    return X_train, Y_train, X_validation, Y_validation, X_test, Y_test, modulation_index\n","#X_train\n","#Y_train\n","#modulation_index"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"uedRgKM6N7Ix","executionInfo":{"status":"ok","timestamp":1629450004197,"user_tz":420,"elapsed":17835,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}}},"source":["with open(data_path + file_name, 'rb') as f:\n","    data = pickle.load(f, encoding = 'latin1')\n","#data"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"edOS7buNPc-F","executionInfo":{"status":"ok","timestamp":1629547070694,"user_tz":420,"elapsed":180,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}}},"source":["# Corruption of input data with zeros\n","def zero_mask(X_train, p):\n","    num = int(X_train.shape[1] * p)\n","    res = X_train.copy()\n","    index = np.array([[i for i in range(X_train.shape[1])] for _ in range(X_train.shape[0])])\n","    for i in range(index.shape[0]):\n","        np.random.shuffle(index[i, :])\n","    \n","    for i in range(res.shape[0]):\n","        res[i, index[i, :num], :] = 0\n","        \n","    return res"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpnGHJgvPud7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629547242387,"user_tz":420,"elapsed":26547,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"1c221895-184b-480b-eb9b-7231e103aab6"},"source":["# set up data\n","X_train, Y_train, X_validation, Y_validation, X_test, Y_test, modulation_index = set_up_data(data_path, file_name)\n","\n","'''X_train = np.moveaxis(X_train, 1, 2)\n","X_validation = np.moveaxis(X_validation, 1, 2)\n","X_test = np.moveaxis(X_test, 1, 2)'''\n","\n","X_train = get_amp_phase(X_train)\n","X_validation = get_amp_phase(X_validation)\n","X_test = get_amp_phase(X_test)\n","\n","#Y_train = Y_train.astype(str)\n","#Y_train = Y_train.astype(np.float)\n","#Y_validation = Y_validation.view(np.int)\n","#Y_test = Y_test.view(np.int)\n","\n","X_train.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(110000, 128, 2)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XX3CpiDPy9o8","executionInfo":{"status":"ok","timestamp":1629464587484,"user_tz":420,"elapsed":177,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"7e64c56d-3735-4996-ef4c-fb59d145a139"},"source":["print(tf.__version__)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["2.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aeRFYnZypzSa","executionInfo":{"status":"ok","timestamp":1629547257263,"user_tz":420,"elapsed":1475,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}}},"source":["# LSTM autoencoder\n","# Encoder with 2-hidden layers h1 and h2\n","encoder_inputs = tf.keras.Input(shape = (X_train.shape[1], X_train.shape[2]),\n","                                name = 'encoder_inputs')\n","\n","encoder_1, state_h_1, state_c_1 = tf.keras.layers.LSTM(units = 32,\n","                                    return_sequences = True,\n","                                    return_state = True,\n","                                    name = 'encoder_1')(encoder_inputs)\n","\n","drop_prob = 0.2\n","drop_1 = tf.keras.layers.Dropout(drop_prob, name = 'drop_1')(encoder_1)\n","\n","encoder_2, state_h_2, state_c_2 = tf.keras.layers.LSTM(units = 32,\n","                                    return_state = True,\n","                                    return_sequences = True,                \n","                                    name = 'encoder_2')(drop_1)\n","# Decoder (just one dense layer)\n","decoder = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2),\n","                                          name = 'decoder')(encoder_2)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"WD1I8HuKP9RU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629547265530,"user_tz":420,"elapsed":180,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"cc2ba0fc-d274-4419-d07c-297f1139e582"},"source":["# 3 Dense layers for classification with bn\n","clf_dropout = 0.2\n","\n","clf_dense_1 = tf.keras.layers.Dense(units = 32,\n","                                    activation = tf.nn.relu,\n","                                    name = 'clf_dense_1')(state_h_2)\n","\n","bn_1 = tf.keras.layers.BatchNormalization(name = 'bn_1')(clf_dense_1)\n","\n","clf_drop_1 = tf.keras.layers.Dropout(clf_dropout, name = 'clf_drop_1')(bn_1)\n","\n","clf_dense_2 = tf.keras.layers.Dense(units = 16,\n","                                    activation = tf.nn.relu,\n","                                    name = 'clf_dense_2')(clf_drop_1)\n","\n","bn_2 = tf.keras.layers.BatchNormalization(name = 'bn_2')(clf_dense_2)\n","\n","clf_drop_2 = tf.keras.layers.Dropout(clf_dropout, name = 'clf_drop_2')(bn_2)\n","\n","clf_dense_3 = tf.keras.layers.Dense(units = modulation_num,\n","                                    name = 'clf_dense_3')(clf_drop_2)\n","\n","softmax = tf.keras.layers.Softmax(name = 'softmax')(clf_dense_3)\n","\n","model = tf.keras.Model(inputs = encoder_inputs, outputs = [decoder, softmax])\n","model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","encoder_inputs (InputLayer)     [(None, 128, 2)]     0                                            \n","__________________________________________________________________________________________________\n","encoder_1 (LSTM)                [(None, 128, 32), (N 4480        encoder_inputs[0][0]             \n","__________________________________________________________________________________________________\n","drop_1 (Dropout)                (None, 128, 32)      0           encoder_1[0][0]                  \n","__________________________________________________________________________________________________\n","encoder_2 (LSTM)                [(None, 128, 32), (N 8320        drop_1[0][0]                     \n","__________________________________________________________________________________________________\n","clf_dense_1 (Dense)             (None, 32)           1056        encoder_2[0][1]                  \n","__________________________________________________________________________________________________\n","bn_1 (BatchNormalization)       (None, 32)           128         clf_dense_1[0][0]                \n","__________________________________________________________________________________________________\n","clf_drop_1 (Dropout)            (None, 32)           0           bn_1[0][0]                       \n","__________________________________________________________________________________________________\n","clf_dense_2 (Dense)             (None, 16)           528         clf_drop_1[0][0]                 \n","__________________________________________________________________________________________________\n","bn_2 (BatchNormalization)       (None, 16)           64          clf_dense_2[0][0]                \n","__________________________________________________________________________________________________\n","clf_drop_2 (Dropout)            (None, 16)           0           bn_2[0][0]                       \n","__________________________________________________________________________________________________\n","clf_dense_3 (Dense)             (None, 11)           187         clf_drop_2[0][0]                 \n","__________________________________________________________________________________________________\n","decoder (TimeDistributed)       (None, 128, 2)       66          encoder_2[0][0]                  \n","__________________________________________________________________________________________________\n","softmax (Softmax)               (None, 11)           0           clf_dense_3[0][0]                \n","==================================================================================================\n","Total params: 14,829\n","Trainable params: 14,733\n","Non-trainable params: 96\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7r19kILjQKGb"},"source":["learning_rate = 10 ** -3\n","lam = 0.1\n","\n","model.compile(loss = ['mean_squared_error', 'categorical_crossentropy'],\n","              loss_weights = [1 - lam, lam],\n","              metrics=['accuracy'],\n","              optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate))\n","\n","best = 0\n","train_acc = []\n","val_acc = []\n","\n","\n","for ite in range(150):\n","    X_train_masked = zero_mask(X_train, 0.1)\n","    print(ite)\n","    history = model.fit(x = X_train,\n","                        y = [X_train, tf.keras.utils.to_categorical(Y_train[:, 0])],\n","                        validation_data = (X_validation, [X_validation, tf.keras.utils.to_categorical(Y_validation[:, 0])]),\n","                        batch_size = 128,\n","                        epochs = 1)\n","    \n","    train_acc.append(history.history['softmax_acc'][0])\n","    val_acc.append(history.history['val_softmax_acc'][0])\n","\n","    if history.history['val_softmax_acc'][0] > best:\n","        best = history.history['val_softmax_acc'][0]\n","        model.save('DAELSTM.h5')\n","\n","    with open('val_result.txt', 'a') as f:\n","        f.write(str(history.history['val_softmax_acc'][0] * 100) + '\\n')\n","        \n","\n","clf = tf.keras.models.load_model('DAELSTM.h5')\n","\n","res = clf.predict(X_test)[1]\n","res = np.argmax(res, axis = 1)\n","test_accuracy = {}\n","for i in range(X_test.shape[0]):\n","    if Y_test[i, 1] not in test_accuracy:\n","        if Y_test[i, 0] == res[i]:\n","            test_accuracy[Y_test[i, 1]] = [1, 1]\n","        else:\n","            test_accuracy[Y_test[i, 1]] = [0, 1]\n","    else:\n","        if Y_test[i, 0] == res[i]:\n","            test_accuracy[Y_test[i, 1]][0] += 1\n","            test_accuracy[Y_test[i, 1]][1] += 1\n","        else:\n","            test_accuracy[Y_test[i, 1]][1] += 1\n","\n","nomi = 0\n","deno = 0\n","for snr in test_accuracy:\n","    nomi += test_accuracy[snr][0]\n","    deno += test_accuracy[snr][1]\n","\n","best = nomi / deno\n","\n","with open('result.txt', 'a') as f:\n","    for item in [test_accuracy[i][0] / test_accuracy[i][1] for i in np.sort(list(test_accuracy))]:\n","        f.write(str(item * 100) + '\\n')\n","        \n","    f.write(str(best * 100) + '\\n')\n","    f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"f6Gn-MYShz_P","executionInfo":{"status":"ok","timestamp":1628762314851,"user_tz":420,"elapsed":205,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"c89f621c-3b21-4e31-d674-d658b9c0f5a6"},"source":["pd.DataFrame(X_train[:,:,0]).head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","      <th>110</th>\n","      <th>111</th>\n","      <th>112</th>\n","      <th>113</th>\n","      <th>114</th>\n","      <th>115</th>\n","      <th>116</th>\n","      <th>117</th>\n","      <th>118</th>\n","      <th>119</th>\n","      <th>120</th>\n","      <th>121</th>\n","      <th>122</th>\n","      <th>123</th>\n","      <th>124</th>\n","      <th>125</th>\n","      <th>126</th>\n","      <th>127</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.005442</td>\n","      <td>0.001532</td>\n","      <td>0.005935</td>\n","      <td>0.008835</td>\n","      <td>0.005926</td>\n","      <td>0.008828</td>\n","      <td>0.004293</td>\n","      <td>0.010320</td>\n","      <td>0.010315</td>\n","      <td>0.007771</td>\n","      <td>0.000677</td>\n","      <td>0.000669</td>\n","      <td>0.004803</td>\n","      <td>0.003490</td>\n","      <td>0.003490</td>\n","      <td>0.004939</td>\n","      <td>0.004267</td>\n","      <td>0.004199</td>\n","      <td>0.005977</td>\n","      <td>0.004230</td>\n","      <td>0.004248</td>\n","      <td>0.005954</td>\n","      <td>0.005956</td>\n","      <td>0.003953</td>\n","      <td>0.003425</td>\n","      <td>0.016244</td>\n","      <td>0.007280</td>\n","      <td>0.007285</td>\n","      <td>0.007361</td>\n","      <td>0.005058</td>\n","      <td>0.006594</td>\n","      <td>0.002416</td>\n","      <td>0.001711</td>\n","      <td>0.004857</td>\n","      <td>0.002784</td>\n","      <td>0.006709</td>\n","      <td>0.006184</td>\n","      <td>0.002207</td>\n","      <td>0.003711</td>\n","      <td>0.004674</td>\n","      <td>...</td>\n","      <td>0.007038</td>\n","      <td>0.006744</td>\n","      <td>0.007333</td>\n","      <td>0.007531</td>\n","      <td>0.008131</td>\n","      <td>0.007733</td>\n","      <td>0.001715</td>\n","      <td>0.007129</td>\n","      <td>0.007129</td>\n","      <td>0.004675</td>\n","      <td>0.005712</td>\n","      <td>0.005714</td>\n","      <td>0.005620</td>\n","      <td>0.002945</td>\n","      <td>0.002399</td>\n","      <td>0.004361</td>\n","      <td>0.004121</td>\n","      <td>0.003713</td>\n","      <td>0.004375</td>\n","      <td>0.005814</td>\n","      <td>0.013477</td>\n","      <td>0.003308</td>\n","      <td>0.000015</td>\n","      <td>0.008740</td>\n","      <td>0.008746</td>\n","      <td>0.002746</td>\n","      <td>0.000964</td>\n","      <td>0.000972</td>\n","      <td>0.006277</td>\n","      <td>0.005526</td>\n","      <td>0.005530</td>\n","      <td>0.012662</td>\n","      <td>0.004447</td>\n","      <td>0.004452</td>\n","      <td>0.005209</td>\n","      <td>0.004513</td>\n","      <td>0.004518</td>\n","      <td>0.004174</td>\n","      <td>0.007069</td>\n","      <td>0.006761</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.003841</td>\n","      <td>0.004248</td>\n","      <td>0.003823</td>\n","      <td>0.004471</td>\n","      <td>0.004133</td>\n","      <td>0.004384</td>\n","      <td>0.004005</td>\n","      <td>0.004444</td>\n","      <td>0.004456</td>\n","      <td>0.004582</td>\n","      <td>0.004383</td>\n","      <td>0.004246</td>\n","      <td>0.004141</td>\n","      <td>0.004308</td>\n","      <td>0.004756</td>\n","      <td>0.005053</td>\n","      <td>0.004555</td>\n","      <td>0.004002</td>\n","      <td>0.004358</td>\n","      <td>0.004910</td>\n","      <td>0.004591</td>\n","      <td>0.004707</td>\n","      <td>0.004084</td>\n","      <td>0.004539</td>\n","      <td>0.004355</td>\n","      <td>0.004441</td>\n","      <td>0.004689</td>\n","      <td>0.004535</td>\n","      <td>0.003474</td>\n","      <td>0.004846</td>\n","      <td>0.004738</td>\n","      <td>0.004023</td>\n","      <td>0.004574</td>\n","      <td>0.004765</td>\n","      <td>0.004536</td>\n","      <td>0.004602</td>\n","      <td>0.004207</td>\n","      <td>0.004466</td>\n","      <td>0.004530</td>\n","      <td>0.004240</td>\n","      <td>...</td>\n","      <td>0.004047</td>\n","      <td>0.004492</td>\n","      <td>0.004249</td>\n","      <td>0.004592</td>\n","      <td>0.004597</td>\n","      <td>0.004477</td>\n","      <td>0.004501</td>\n","      <td>0.004325</td>\n","      <td>0.004289</td>\n","      <td>0.004827</td>\n","      <td>0.004194</td>\n","      <td>0.004187</td>\n","      <td>0.004522</td>\n","      <td>0.004156</td>\n","      <td>0.004944</td>\n","      <td>0.004137</td>\n","      <td>0.004297</td>\n","      <td>0.005210</td>\n","      <td>0.004161</td>\n","      <td>0.004749</td>\n","      <td>0.004530</td>\n","      <td>0.004639</td>\n","      <td>0.004323</td>\n","      <td>0.005120</td>\n","      <td>0.004230</td>\n","      <td>0.004243</td>\n","      <td>0.004500</td>\n","      <td>0.004553</td>\n","      <td>0.003970</td>\n","      <td>0.004232</td>\n","      <td>0.004615</td>\n","      <td>0.004539</td>\n","      <td>0.004248</td>\n","      <td>0.004331</td>\n","      <td>0.004399</td>\n","      <td>0.004364</td>\n","      <td>0.004506</td>\n","      <td>0.004506</td>\n","      <td>0.004519</td>\n","      <td>0.004231</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.008249</td>\n","      <td>0.006606</td>\n","      <td>0.005395</td>\n","      <td>0.003387</td>\n","      <td>0.000742</td>\n","      <td>-0.001238</td>\n","      <td>-0.001964</td>\n","      <td>-0.002642</td>\n","      <td>-0.002714</td>\n","      <td>-0.002072</td>\n","      <td>-0.000574</td>\n","      <td>0.001242</td>\n","      <td>0.003180</td>\n","      <td>0.005252</td>\n","      <td>0.006871</td>\n","      <td>0.007972</td>\n","      <td>0.008358</td>\n","      <td>0.007662</td>\n","      <td>0.006191</td>\n","      <td>0.004763</td>\n","      <td>0.003669</td>\n","      <td>0.000826</td>\n","      <td>-0.000484</td>\n","      <td>-0.003147</td>\n","      <td>-0.004135</td>\n","      <td>-0.004124</td>\n","      <td>-0.003133</td>\n","      <td>-0.003319</td>\n","      <td>-0.002663</td>\n","      <td>-0.001754</td>\n","      <td>-0.002178</td>\n","      <td>-0.002113</td>\n","      <td>-0.001135</td>\n","      <td>-0.002687</td>\n","      <td>-0.003274</td>\n","      <td>-0.005171</td>\n","      <td>-0.005987</td>\n","      <td>-0.007156</td>\n","      <td>-0.008185</td>\n","      <td>-0.008339</td>\n","      <td>...</td>\n","      <td>0.008172</td>\n","      <td>0.007089</td>\n","      <td>0.005224</td>\n","      <td>0.003661</td>\n","      <td>0.000025</td>\n","      <td>-0.003115</td>\n","      <td>-0.005906</td>\n","      <td>-0.008115</td>\n","      <td>-0.009691</td>\n","      <td>-0.010335</td>\n","      <td>-0.010177</td>\n","      <td>-0.009398</td>\n","      <td>-0.009329</td>\n","      <td>-0.007776</td>\n","      <td>-0.006453</td>\n","      <td>-0.006331</td>\n","      <td>-0.003826</td>\n","      <td>-0.005019</td>\n","      <td>-0.004747</td>\n","      <td>-0.005977</td>\n","      <td>-0.005974</td>\n","      <td>-0.005722</td>\n","      <td>-0.004361</td>\n","      <td>-0.003722</td>\n","      <td>-0.002114</td>\n","      <td>-0.000151</td>\n","      <td>0.001483</td>\n","      <td>0.002581</td>\n","      <td>0.004227</td>\n","      <td>0.005304</td>\n","      <td>0.006214</td>\n","      <td>0.006213</td>\n","      <td>0.006475</td>\n","      <td>0.006355</td>\n","      <td>0.006443</td>\n","      <td>0.004558</td>\n","      <td>0.005137</td>\n","      <td>0.004594</td>\n","      <td>0.004170</td>\n","      <td>0.003955</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.002089</td>\n","      <td>0.001235</td>\n","      <td>-0.000165</td>\n","      <td>-0.001581</td>\n","      <td>-0.002995</td>\n","      <td>-0.004830</td>\n","      <td>-0.005847</td>\n","      <td>-0.007003</td>\n","      <td>-0.007312</td>\n","      <td>-0.007647</td>\n","      <td>-0.007360</td>\n","      <td>-0.006859</td>\n","      <td>-0.006231</td>\n","      <td>-0.005830</td>\n","      <td>-0.005523</td>\n","      <td>-0.005480</td>\n","      <td>-0.006289</td>\n","      <td>-0.007302</td>\n","      <td>-0.008586</td>\n","      <td>-0.009856</td>\n","      <td>-0.011157</td>\n","      <td>-0.011522</td>\n","      <td>-0.011638</td>\n","      <td>-0.010313</td>\n","      <td>-0.008619</td>\n","      <td>-0.006088</td>\n","      <td>-0.002861</td>\n","      <td>0.000375</td>\n","      <td>0.004281</td>\n","      <td>0.006864</td>\n","      <td>0.008436</td>\n","      <td>0.009529</td>\n","      <td>0.009500</td>\n","      <td>0.008018</td>\n","      <td>0.006610</td>\n","      <td>0.004225</td>\n","      <td>0.001965</td>\n","      <td>0.000317</td>\n","      <td>-0.000697</td>\n","      <td>-0.001461</td>\n","      <td>...</td>\n","      <td>0.007993</td>\n","      <td>0.009766</td>\n","      <td>0.010867</td>\n","      <td>0.011659</td>\n","      <td>0.011649</td>\n","      <td>0.011008</td>\n","      <td>0.009810</td>\n","      <td>0.007983</td>\n","      <td>0.005409</td>\n","      <td>0.002693</td>\n","      <td>-0.000187</td>\n","      <td>-0.003153</td>\n","      <td>-0.005661</td>\n","      <td>-0.007863</td>\n","      <td>-0.008704</td>\n","      <td>-0.009432</td>\n","      <td>-0.009122</td>\n","      <td>-0.008022</td>\n","      <td>-0.006544</td>\n","      <td>-0.004786</td>\n","      <td>-0.002729</td>\n","      <td>-0.000799</td>\n","      <td>0.000891</td>\n","      <td>0.002563</td>\n","      <td>0.003913</td>\n","      <td>0.004956</td>\n","      <td>0.005504</td>\n","      <td>0.006383</td>\n","      <td>0.006754</td>\n","      <td>0.007299</td>\n","      <td>0.007390</td>\n","      <td>0.007687</td>\n","      <td>0.007295</td>\n","      <td>0.006715</td>\n","      <td>0.006240</td>\n","      <td>0.005144</td>\n","      <td>0.004354</td>\n","      <td>0.003886</td>\n","      <td>0.003512</td>\n","      <td>0.003327</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.001474</td>\n","      <td>0.001372</td>\n","      <td>0.002916</td>\n","      <td>0.005375</td>\n","      <td>0.009456</td>\n","      <td>0.008601</td>\n","      <td>0.012096</td>\n","      <td>0.012273</td>\n","      <td>0.011755</td>\n","      <td>0.012510</td>\n","      <td>0.012175</td>\n","      <td>0.011334</td>\n","      <td>0.009381</td>\n","      <td>0.007544</td>\n","      <td>0.006286</td>\n","      <td>0.006356</td>\n","      <td>0.004116</td>\n","      <td>0.002171</td>\n","      <td>0.001972</td>\n","      <td>0.000920</td>\n","      <td>0.000912</td>\n","      <td>-0.001345</td>\n","      <td>0.001128</td>\n","      <td>-0.000282</td>\n","      <td>0.001294</td>\n","      <td>0.002560</td>\n","      <td>0.004554</td>\n","      <td>0.006114</td>\n","      <td>0.006659</td>\n","      <td>0.009224</td>\n","      <td>0.009124</td>\n","      <td>0.010843</td>\n","      <td>0.011277</td>\n","      <td>0.010462</td>\n","      <td>0.008762</td>\n","      <td>0.006928</td>\n","      <td>0.004387</td>\n","      <td>0.004246</td>\n","      <td>0.000774</td>\n","      <td>-0.000824</td>\n","      <td>...</td>\n","      <td>-0.001502</td>\n","      <td>-0.002060</td>\n","      <td>-0.003806</td>\n","      <td>-0.004085</td>\n","      <td>-0.003839</td>\n","      <td>-0.005990</td>\n","      <td>-0.006100</td>\n","      <td>-0.007145</td>\n","      <td>-0.006133</td>\n","      <td>-0.004282</td>\n","      <td>-0.004224</td>\n","      <td>-0.001284</td>\n","      <td>-0.000307</td>\n","      <td>0.001885</td>\n","      <td>0.004583</td>\n","      <td>0.006099</td>\n","      <td>0.005920</td>\n","      <td>0.007797</td>\n","      <td>0.009120</td>\n","      <td>0.007556</td>\n","      <td>0.007303</td>\n","      <td>0.004645</td>\n","      <td>0.003032</td>\n","      <td>0.003625</td>\n","      <td>0.001582</td>\n","      <td>0.000132</td>\n","      <td>-0.001447</td>\n","      <td>-0.001101</td>\n","      <td>-0.002680</td>\n","      <td>-0.003977</td>\n","      <td>-0.007771</td>\n","      <td>-0.006129</td>\n","      <td>-0.007372</td>\n","      <td>-0.009607</td>\n","      <td>-0.009531</td>\n","      <td>-0.010373</td>\n","      <td>-0.012678</td>\n","      <td>-0.012660</td>\n","      <td>-0.013066</td>\n","      <td>-0.012490</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 128 columns</p>\n","</div>"],"text/plain":["        0         1         2    ...       125       126       127\n","0  0.005442  0.001532  0.005935  ...  0.004174  0.007069  0.006761\n","1  0.003841  0.004248  0.003823  ...  0.004506  0.004519  0.004231\n","2  0.008249  0.006606  0.005395  ...  0.004594  0.004170  0.003955\n","3  0.002089  0.001235 -0.000165  ...  0.003886  0.003512  0.003327\n","4  0.001474  0.001372  0.002916  ... -0.012660 -0.013066 -0.012490\n","\n","[5 rows x 128 columns]"]},"metadata":{"tags":[]},"execution_count":77}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":253},"id":"qVNpnoRYiuSM","executionInfo":{"status":"ok","timestamp":1628758149828,"user_tz":420,"elapsed":185,"user":{"displayName":"Amadou Tall","photoUrl":"","userId":"06820629351691459518"}},"outputId":"5f4766da-5508-435e-8ba9-c8d8248179b4"},"source":["pd.DataFrame(X_train[:,:,1]).head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>...</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>72</th>\n","      <th>73</th>\n","      <th>74</th>\n","      <th>75</th>\n","      <th>76</th>\n","      <th>77</th>\n","      <th>78</th>\n","      <th>79</th>\n","      <th>80</th>\n","      <th>81</th>\n","      <th>82</th>\n","      <th>83</th>\n","      <th>84</th>\n","      <th>85</th>\n","      <th>86</th>\n","      <th>87</th>\n","      <th>88</th>\n","      <th>89</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","      <th>101</th>\n","      <th>102</th>\n","      <th>103</th>\n","      <th>104</th>\n","      <th>105</th>\n","      <th>106</th>\n","      <th>107</th>\n","      <th>108</th>\n","      <th>109</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.087368</td>\n","      <td>0.139354</td>\n","      <td>0.265992</td>\n","      <td>-0.740829</td>\n","      <td>0.214924</td>\n","      <td>0.306048</td>\n","      <td>0.169925</td>\n","      <td>-0.791247</td>\n","      <td>0.238630</td>\n","      <td>0.674254</td>\n","      <td>0.765281</td>\n","      <td>0.105232</td>\n","      <td>0.222824</td>\n","      <td>-0.736672</td>\n","      <td>0.894721</td>\n","      <td>-0.916096</td>\n","      <td>-0.208604</td>\n","      <td>-0.330005</td>\n","      <td>0.280239</td>\n","      <td>-0.639442</td>\n","      <td>-0.756895</td>\n","      <td>0.277411</td>\n","      <td>0.904680</td>\n","      <td>0.668013</td>\n","      <td>-0.779246</td>\n","      <td>-0.765078</td>\n","      <td>-0.780375</td>\n","      <td>0.248668</td>\n","      <td>-0.123996</td>\n","      <td>0.606725</td>\n","      <td>0.160394</td>\n","      <td>-0.702527</td>\n","      <td>0.501910</td>\n","      <td>0.636676</td>\n","      <td>0.707380</td>\n","      <td>-0.675575</td>\n","      <td>-0.735531</td>\n","      <td>-0.570908</td>\n","      <td>-0.018375</td>\n","      <td>-0.833218</td>\n","      <td>...</td>\n","      <td>-0.750003</td>\n","      <td>-0.750006</td>\n","      <td>-0.857458</td>\n","      <td>0.736389</td>\n","      <td>-0.687286</td>\n","      <td>-0.775132</td>\n","      <td>-0.725620</td>\n","      <td>-0.652883</td>\n","      <td>0.251386</td>\n","      <td>-0.749844</td>\n","      <td>0.242447</td>\n","      <td>-0.755785</td>\n","      <td>-0.213606</td>\n","      <td>-0.521999</td>\n","      <td>0.099891</td>\n","      <td>-0.043232</td>\n","      <td>0.247138</td>\n","      <td>0.245454</td>\n","      <td>-0.420991</td>\n","      <td>0.829698</td>\n","      <td>0.301118</td>\n","      <td>0.239212</td>\n","      <td>0.717248</td>\n","      <td>-0.049377</td>\n","      <td>-0.657449</td>\n","      <td>0.657898</td>\n","      <td>0.262644</td>\n","      <td>0.361919</td>\n","      <td>-0.859132</td>\n","      <td>-0.741591</td>\n","      <td>-0.855887</td>\n","      <td>-0.786086</td>\n","      <td>0.983559</td>\n","      <td>-0.526376</td>\n","      <td>-0.700581</td>\n","      <td>-0.752575</td>\n","      <td>0.228411</td>\n","      <td>-0.900165</td>\n","      <td>0.162871</td>\n","      <td>0.546614</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.346174</td>\n","      <td>0.111799</td>\n","      <td>0.248782</td>\n","      <td>0.243412</td>\n","      <td>-0.265765</td>\n","      <td>-0.282332</td>\n","      <td>0.460824</td>\n","      <td>-0.748499</td>\n","      <td>-0.916491</td>\n","      <td>-0.112533</td>\n","      <td>-0.813976</td>\n","      <td>-0.617083</td>\n","      <td>0.345973</td>\n","      <td>-0.762705</td>\n","      <td>-0.703368</td>\n","      <td>-0.743288</td>\n","      <td>0.266034</td>\n","      <td>-0.752250</td>\n","      <td>0.265528</td>\n","      <td>0.277692</td>\n","      <td>-0.708491</td>\n","      <td>-0.776674</td>\n","      <td>0.246392</td>\n","      <td>-0.723704</td>\n","      <td>-0.755453</td>\n","      <td>-0.619199</td>\n","      <td>0.431006</td>\n","      <td>0.379734</td>\n","      <td>0.418324</td>\n","      <td>-0.745777</td>\n","      <td>0.488580</td>\n","      <td>-0.735368</td>\n","      <td>-0.257103</td>\n","      <td>-0.754680</td>\n","      <td>0.107881</td>\n","      <td>-0.573662</td>\n","      <td>0.599034</td>\n","      <td>-0.304726</td>\n","      <td>0.246907</td>\n","      <td>-0.746980</td>\n","      <td>...</td>\n","      <td>-0.729234</td>\n","      <td>0.195075</td>\n","      <td>-0.638548</td>\n","      <td>0.246553</td>\n","      <td>0.151533</td>\n","      <td>-0.743194</td>\n","      <td>-0.789748</td>\n","      <td>-0.940368</td>\n","      <td>0.259925</td>\n","      <td>-0.737891</td>\n","      <td>0.474511</td>\n","      <td>-0.465951</td>\n","      <td>0.263525</td>\n","      <td>-0.692659</td>\n","      <td>0.075735</td>\n","      <td>-0.823399</td>\n","      <td>-0.893832</td>\n","      <td>0.402240</td>\n","      <td>0.251911</td>\n","      <td>-0.745847</td>\n","      <td>-0.720464</td>\n","      <td>-0.741127</td>\n","      <td>-0.560299</td>\n","      <td>0.123437</td>\n","      <td>0.834682</td>\n","      <td>0.665441</td>\n","      <td>0.519304</td>\n","      <td>-0.649738</td>\n","      <td>0.911739</td>\n","      <td>0.515071</td>\n","      <td>0.255132</td>\n","      <td>0.300276</td>\n","      <td>-0.236178</td>\n","      <td>-0.607603</td>\n","      <td>-0.736866</td>\n","      <td>-0.311693</td>\n","      <td>-0.757137</td>\n","      <td>0.217876</td>\n","      <td>-0.763222</td>\n","      <td>0.229711</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.312185</td>\n","      <td>0.438581</td>\n","      <td>0.471968</td>\n","      <td>0.480735</td>\n","      <td>-0.704118</td>\n","      <td>0.224675</td>\n","      <td>0.343816</td>\n","      <td>0.109358</td>\n","      <td>0.181895</td>\n","      <td>0.565094</td>\n","      <td>-0.142019</td>\n","      <td>-0.700207</td>\n","      <td>-0.741964</td>\n","      <td>-0.709749</td>\n","      <td>-0.608795</td>\n","      <td>-0.228001</td>\n","      <td>0.370390</td>\n","      <td>-0.115898</td>\n","      <td>0.987572</td>\n","      <td>0.142160</td>\n","      <td>-0.731468</td>\n","      <td>-0.807147</td>\n","      <td>0.505105</td>\n","      <td>0.639932</td>\n","      <td>-0.628694</td>\n","      <td>-0.330894</td>\n","      <td>-0.536734</td>\n","      <td>0.265650</td>\n","      <td>0.295428</td>\n","      <td>0.736745</td>\n","      <td>-0.051859</td>\n","      <td>0.234996</td>\n","      <td>0.801309</td>\n","      <td>-0.114693</td>\n","      <td>0.255122</td>\n","      <td>-0.074144</td>\n","      <td>0.510949</td>\n","      <td>-0.261068</td>\n","      <td>-0.571571</td>\n","      <td>-0.744510</td>\n","      <td>...</td>\n","      <td>-0.377897</td>\n","      <td>0.985627</td>\n","      <td>0.492738</td>\n","      <td>0.498936</td>\n","      <td>-0.699347</td>\n","      <td>-0.020626</td>\n","      <td>-0.746589</td>\n","      <td>-0.767505</td>\n","      <td>-0.744036</td>\n","      <td>-0.749822</td>\n","      <td>-0.760787</td>\n","      <td>-0.726189</td>\n","      <td>0.166976</td>\n","      <td>-0.736771</td>\n","      <td>0.282542</td>\n","      <td>0.296020</td>\n","      <td>0.758139</td>\n","      <td>-0.840935</td>\n","      <td>0.329640</td>\n","      <td>-0.781005</td>\n","      <td>-0.030735</td>\n","      <td>-0.845780</td>\n","      <td>-0.756035</td>\n","      <td>-0.527382</td>\n","      <td>0.098005</td>\n","      <td>-0.599431</td>\n","      <td>-0.317517</td>\n","      <td>0.471673</td>\n","      <td>0.680371</td>\n","      <td>0.852118</td>\n","      <td>0.261623</td>\n","      <td>0.237184</td>\n","      <td>0.244196</td>\n","      <td>0.252975</td>\n","      <td>-0.825310</td>\n","      <td>-0.734064</td>\n","      <td>0.425755</td>\n","      <td>-0.556169</td>\n","      <td>-0.703945</td>\n","      <td>0.020343</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.111349</td>\n","      <td>-0.655143</td>\n","      <td>-0.857042</td>\n","      <td>-0.336923</td>\n","      <td>-0.553280</td>\n","      <td>-0.064774</td>\n","      <td>0.178789</td>\n","      <td>-0.058962</td>\n","      <td>0.246213</td>\n","      <td>0.399913</td>\n","      <td>0.639193</td>\n","      <td>-0.176869</td>\n","      <td>0.536541</td>\n","      <td>-0.835214</td>\n","      <td>0.252599</td>\n","      <td>-0.756101</td>\n","      <td>0.526020</td>\n","      <td>-0.833677</td>\n","      <td>0.205213</td>\n","      <td>0.309335</td>\n","      <td>-0.191957</td>\n","      <td>-0.720776</td>\n","      <td>-0.632293</td>\n","      <td>0.751311</td>\n","      <td>-0.737933</td>\n","      <td>0.165187</td>\n","      <td>0.538008</td>\n","      <td>-0.487160</td>\n","      <td>0.229245</td>\n","      <td>0.019981</td>\n","      <td>0.376860</td>\n","      <td>-0.730841</td>\n","      <td>-0.197804</td>\n","      <td>0.517082</td>\n","      <td>-0.817512</td>\n","      <td>-0.648098</td>\n","      <td>0.741716</td>\n","      <td>-0.241206</td>\n","      <td>0.404624</td>\n","      <td>0.346671</td>\n","      <td>...</td>\n","      <td>-0.712009</td>\n","      <td>-0.543609</td>\n","      <td>0.890487</td>\n","      <td>0.750597</td>\n","      <td>-0.280679</td>\n","      <td>0.004412</td>\n","      <td>-0.647065</td>\n","      <td>0.416625</td>\n","      <td>-0.855912</td>\n","      <td>-0.737560</td>\n","      <td>-0.843781</td>\n","      <td>-0.768739</td>\n","      <td>-0.748332</td>\n","      <td>0.783303</td>\n","      <td>0.271732</td>\n","      <td>0.220810</td>\n","      <td>-0.756610</td>\n","      <td>-0.801271</td>\n","      <td>-0.777272</td>\n","      <td>0.224549</td>\n","      <td>-0.859340</td>\n","      <td>-0.606112</td>\n","      <td>0.819358</td>\n","      <td>-0.921745</td>\n","      <td>-0.842829</td>\n","      <td>0.001305</td>\n","      <td>-0.758094</td>\n","      <td>-0.257625</td>\n","      <td>-0.733302</td>\n","      <td>0.252698</td>\n","      <td>0.142776</td>\n","      <td>0.670851</td>\n","      <td>-0.877926</td>\n","      <td>-0.110389</td>\n","      <td>-0.765919</td>\n","      <td>0.310934</td>\n","      <td>0.294922</td>\n","      <td>-0.735556</td>\n","      <td>0.255090</td>\n","      <td>-0.754613</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.302932</td>\n","      <td>-0.241332</td>\n","      <td>-0.605497</td>\n","      <td>-0.701021</td>\n","      <td>-0.206909</td>\n","      <td>-0.620864</td>\n","      <td>0.941024</td>\n","      <td>0.096202</td>\n","      <td>0.253873</td>\n","      <td>0.975769</td>\n","      <td>-0.751516</td>\n","      <td>-0.741011</td>\n","      <td>0.243562</td>\n","      <td>-0.492074</td>\n","      <td>-0.755552</td>\n","      <td>-0.778018</td>\n","      <td>-0.753137</td>\n","      <td>-0.704775</td>\n","      <td>-0.709483</td>\n","      <td>0.268477</td>\n","      <td>-0.897839</td>\n","      <td>0.378697</td>\n","      <td>-0.015874</td>\n","      <td>-0.625520</td>\n","      <td>-0.372603</td>\n","      <td>-0.125884</td>\n","      <td>-0.041559</td>\n","      <td>-0.524199</td>\n","      <td>0.911866</td>\n","      <td>0.956760</td>\n","      <td>-0.680146</td>\n","      <td>0.270724</td>\n","      <td>0.192488</td>\n","      <td>0.006155</td>\n","      <td>0.206148</td>\n","      <td>0.944033</td>\n","      <td>-0.026218</td>\n","      <td>-0.242742</td>\n","      <td>-0.964364</td>\n","      <td>0.586723</td>\n","      <td>...</td>\n","      <td>0.278423</td>\n","      <td>0.399394</td>\n","      <td>-0.137750</td>\n","      <td>0.575116</td>\n","      <td>0.841093</td>\n","      <td>-0.126874</td>\n","      <td>0.341127</td>\n","      <td>0.253103</td>\n","      <td>0.541091</td>\n","      <td>0.381881</td>\n","      <td>-0.736981</td>\n","      <td>-0.863679</td>\n","      <td>0.296939</td>\n","      <td>-0.608749</td>\n","      <td>0.747347</td>\n","      <td>0.608165</td>\n","      <td>-0.650156</td>\n","      <td>0.518152</td>\n","      <td>0.987825</td>\n","      <td>0.609147</td>\n","      <td>-0.842479</td>\n","      <td>-0.727025</td>\n","      <td>0.249937</td>\n","      <td>-0.820086</td>\n","      <td>-0.686946</td>\n","      <td>-0.123242</td>\n","      <td>0.948460</td>\n","      <td>-0.923370</td>\n","      <td>-0.237755</td>\n","      <td>-0.163610</td>\n","      <td>0.112055</td>\n","      <td>-0.730533</td>\n","      <td>-0.671696</td>\n","      <td>0.231439</td>\n","      <td>-0.773237</td>\n","      <td>0.084981</td>\n","      <td>0.333960</td>\n","      <td>0.356115</td>\n","      <td>0.255285</td>\n","      <td>0.253627</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 110 columns</p>\n","</div>"],"text/plain":["        0         1         2    ...       107       108       109\n","0  0.087368  0.139354  0.265992  ... -0.900165  0.162871  0.546614\n","1  0.346174  0.111799  0.248782  ...  0.217876 -0.763222  0.229711\n","2  0.312185  0.438581  0.471968  ... -0.556169 -0.703945  0.020343\n","3  0.111349 -0.655143 -0.857042  ... -0.735556  0.255090 -0.754613\n","4 -0.302932 -0.241332 -0.605497  ...  0.356115  0.255285  0.253627\n","\n","[5 rows x 110 columns]"]},"metadata":{"tags":[]},"execution_count":30}]}]}